{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing torch\n",
    "!pip install -q torch einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the parameters\n",
    "patch_size = 16\n",
    "embedding_dim = 128\n",
    "num_channels = 3\n",
    "num_heads = 8\n",
    "image_size = 224\n",
    "batch_size = 10\n",
    "dropout = 0.1\n",
    "n_heads = 8\n",
    "head_dim = 64\n",
    "embedding_dim = 128\n",
    "# feed forward layer parameters\n",
    "ff_hidden_dim = 256\n",
    "ff_dropout_val = 0.1\n",
    "num_layers = 6\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "  def __init__(self, image_height = image_size, image_width = image_size, patch_height = patch_size, patch_width = patch_size, num_channels = num_channels, embed_dim = embedding_dim):\n",
    "    super().__init__()\n",
    "\n",
    "    self.image_height = image_height\n",
    "    self.image_width = image_width\n",
    "    self.patch_height = patch_height\n",
    "    self.patch_width = patch_width\n",
    "    self.num_channels = num_channels\n",
    "    self.embed_dim = embed_dim\n",
    "    print(f\"Embedding dimension: {self.embed_dim}\")\n",
    "\n",
    "    self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "    print(f\"Number of patches: {self.num_patches}\")\n",
    "\n",
    "    #patch dimension\n",
    "    self.patch_dimension = self.patch_height * self.patch_width * self.num_channels\n",
    "    print(f\"Patch dimension: patch_height * patch_width * num_channels =  {self.patch_dimension}\")\n",
    "\n",
    "    self.cls_token = nn.Parameter(torch.randn(self.embed_dim))\n",
    "\n",
    "    self.patch_embed = nn.Sequential(\n",
    "            # This pre and post layer norm speeds up convergence\n",
    "            # Comment them if you want pure vit implementation\n",
    "            nn.LayerNorm(self.patch_dimension),\n",
    "            nn.Linear(self.patch_dimension, self.embed_dim),\n",
    "            nn.LayerNorm(self.embed_dim)\n",
    "        )\n",
    "\n",
    "    self.positional_embedding = nn.Parameter(torch.zeros(1,self.num_patches +1, self.embed_dim))\n",
    "    self.patch_embed_drop = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    # rearranging the input\n",
    "    out = rearrange(x, 'b c (nh ph) (nw pw) -> b (nh nw) (ph pw c)',\n",
    "                      ph=self.patch_height,\n",
    "                      pw=self.patch_width)\n",
    "    print(f\"Rearranged shape: {out.shape}\")\n",
    "\n",
    "    # embedding the patches\n",
    "    embed_out = self.patch_embed(out)\n",
    "    print(f\"Embedding layer :\\nInput shape: {out.shape}\\nOutput shape: {embed_out.shape}\")\n",
    "\n",
    "    # adding cls token\n",
    "    cls_tokens = repeat(self.cls_token, 'd -> b 1 d', b=batch_size)\n",
    "    print(f\"CLS token shape: {cls_tokens.shape}\")\n",
    "\n",
    "    embed_out = torch.cat((cls_tokens, embed_out), dim=1)\n",
    "    print(f\"cls token : {cls_tokens.shape} + embedding : {embed_out.shape} = output : {embed_out.shape}\")\n",
    "\n",
    "    # adding positional embedding\n",
    "    embed_out += self.positional_embedding\n",
    "    print(f\"positional embedding shape : {self.positional_embedding.shape}, output shape : {embed_out.shape}\")\n",
    "\n",
    "    # dropout\n",
    "    embed_out = self.patch_embed_drop(embed_out)\n",
    "\n",
    "    return embed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "  def __init__(self, n_heads=n_heads, head_dim = head_dim, embedding_dim = embedding_dim):\n",
    "    super().__init__()\n",
    "\n",
    "    self.n_heads = n_heads\n",
    "    self.head_dim = head_dim\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    # attention dimension\n",
    "    self.attention_dim = self.n_heads * self.head_dim\n",
    "    print(f\"Attention dimension: no of heads * head dimension =  {self.attention_dim}\")\n",
    "\n",
    "    # key query value\n",
    "    self.qkv_projection = nn.Linear(self.embedding_dim, 3*self.attention_dim, bias = False)\n",
    "\n",
    "    # final output projection\n",
    "    self.projection_out = nn.Sequential(\n",
    "        nn.Linear(self.attention_dim, self.embedding_dim),\n",
    "        nn.Dropout(0.1)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "\n",
    "    # saving batch size and number of channels\n",
    "    batch_size, num_channels = x.shape[:2]\n",
    "\n",
    "    # changing the input dimension (embedding dimension) to attention dimension*3\n",
    "    qkv_out = self.qkv_projection(x)\n",
    "    print(f\"QKV dimension change :\\nInput shape: {x.shape}\\nOutput shape: {qkv_out.shape}\")\n",
    "\n",
    "    # splitting the output into q k v matrix\n",
    "    q, k , v = qkv_out.split(self.attention_dim, dim = -1)\n",
    "    print(f\"q k v splitting :\\ninput shape : {qkv_out.shape}\\nshape of each q k v : {q.shape} \")\n",
    "\n",
    "    # rearranging q k v for processing\n",
    "    q_new = rearrange(q, 'b n (n_h h_dim) -> b n_h n h_dim',\n",
    "                      n_h=self.n_heads, h_dim=self.head_dim)\n",
    "    print(f\"Rearranged q shape :\\nbefore : {q.shape}\\nafter : {q_new.shape} [batch_size, no_of_head, no_of_patch, head_dim]\")   # [10, 8, 197, 64] = [batch_size, no_of_head, no_of_patch, head_dim]\n",
    "\n",
    "    k_new = rearrange(k, 'b n (n_h h_dim) -> b n_h n h_dim',\n",
    "                      n_h=self.n_heads, h_dim=self.head_dim)\n",
    "    print(f\"Rearranged k shape :\\nbefore : {k.shape}\\nafter : {k_new.shape} [batch_size, no_of_head, no_of_patch, head_dim]\")\n",
    "\n",
    "    v_new = rearrange(v, 'b n (n_h h_dim) -> b n_h n h_dim',\n",
    "                      n_h=self.n_heads, h_dim=self.head_dim)\n",
    "    print(f\"Rearranged v shape :\\nbefore : {v.shape}\\nafter : {v_new.shape} [batch_size, no_of_head, no_of_patch, head_dim]\")\n",
    "\n",
    "    # attention weight calculation\n",
    "    att = torch.matmul(q_new,k_new.transpose(-2,-1))*(self.head_dim**-0.5)\n",
    "    print(f\"(q * K)*(head_dim**-0.5):Output shape: {att.shape}\")\n",
    "\n",
    "    # passing through softmax layer\n",
    "    att = torch.nn.functional.softmax(att, dim=-1)\n",
    "    print(f\"After passing through softmax :Output shape: {att.shape}\")\n",
    "\n",
    "    # softmax layer output * v\n",
    "    out = torch.matmul(att, v_new)\n",
    "    print(f\"Softmax_out * V :Output shape: {out.shape}\")\n",
    "\n",
    "    # B x N x (Heads * Head Dimension) -> B x N x (Attention Dimension)\n",
    "    out_final = rearrange(out, 'b n_h n h_dim -> b n (n_h h_dim)')\n",
    "    print(f\"Rearranged out shape :\\nbefore : {out.shape}\\nafter : {out_final.shape}\")\n",
    "\n",
    "    # final reshaping\n",
    "    out_attention = self.projection_out(out_final)\n",
    "    print(f\"Final output shape :\\nInput shape: {out_final.shape}\\nOutput shape: {out_attention.shape}\")\n",
    "\n",
    "    return out_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer layer\n",
    "class TransformerLayer(nn.Module):\n",
    "  def __init__(self,ff_hidden_dim = ff_hidden_dim, embed_dim = embedding_dim,ff_dropout_val = ff_dropout_val):\n",
    "    super().__init__()\n",
    "\n",
    "    self.ff_dim = ff_hidden_dim\n",
    "    self.embed_dim = embed_dim\n",
    "    self.ff_dropout_val = ff_dropout_val\n",
    "\n",
    "    # normalization layer\n",
    "    self.normalization_layer = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    # attention block\n",
    "    self.attention = Attention()\n",
    "\n",
    "    # feed forward layer\n",
    "    self.ff_block = nn.Sequential(\n",
    "        nn.Linear(self.embed_dim, self.ff_dim),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(self.ff_dropout_val),\n",
    "        nn.Linear(self.ff_dim, self.embed_dim),\n",
    "        nn.Dropout(self.ff_dropout_val)\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "\n",
    "    input_val = x\n",
    "\n",
    "    # attention block\n",
    "    out_attention =self.attention(self.normalization_layer(input_val))+input_val\n",
    "    print(f\"Passing through attention block + residual connection:\\nInput shape: {input_val.shape}\\nOutput shape: {out_attention.shape}\")\n",
    "\n",
    "    # feed forward block\n",
    "    out_transformLayer = self.ff_block(self.normalization_layer(out_attention))+out_attention\n",
    "    print(f\"Passing through feed forward block + residual connection:\\nInput shape: {out_attention.shape}\\nOutput shape: {out_transformLayer.shape}\")\n",
    "\n",
    "    return out_transformLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full ViT\n",
    "class ViT(nn.Module):\n",
    "  def __init__(self, n_layers = num_layers, n_class = num_classes, embed_dim = embedding_dim):\n",
    "    super().__init__()\n",
    "\n",
    "    self.n_layers = n_layers\n",
    "    self.n_classes = n_class\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    # patch embedding layer\n",
    "    self.patchLayer = PatchEmbed()\n",
    "\n",
    "    # transformer layers\n",
    "    self.transformerLayers = nn.ModuleList([TransformerLayer() for _ in range(self.n_layers)])\n",
    "\n",
    "    # normalization layer\n",
    "    self.normal = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    # classification output layer\n",
    "    self.classificationLayer = nn.Linear(self.embed_dim, self.n_classes)\n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "\n",
    "    print(f\"Input shape to VIT: {x.shape}\")\n",
    "    # patching the image\n",
    "    out = self.patchLayer(x)\n",
    "    print(f\"Output from PATCH LAYER shape: {out.shape}\")\n",
    "\n",
    "    # passing through transformer layers, looping through them\n",
    "    for layer in self.transformerLayers:\n",
    "      out = layer(out)\n",
    "\n",
    "    print(f\"Output after passing through TransformerLayers : {out.shape}\")\n",
    "    # normalization\n",
    "    out = self.normal(out)\n",
    "\n",
    "    # classification layer\n",
    "    classification_out = self.classificationLayer(out[:,0])\n",
    "    print(f\"Output after passing through classification layer : {classification_out.shape}\")\n",
    "\n",
    "    return classification_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_object = torch.randn([10,3,224,224])\n",
    "vit_obj = ViT()\n",
    "vit_out = vit_obj(test_batch_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
